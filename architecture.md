# ECHO V3 - COMPLETE SYSTEM ARCHITECTURE
## The Developer's Bible for Understanding, Debugging & Extending

**Last Updated:** 2026-01-18 13:07:21 UTC  
**Version:** 3.0.0  
**Status:** Production Ready  
**Maintained By:** AI Development Team

---

## TABLE OF CONTENTS
1. [System Overview](#system-overview)
2. [Core Architecture](#core-architecture)
3. [Services & Ports](#services--ports)
4. [Data Flow Diagrams](#data-flow-diagrams)
5. [Module Reference](#module-reference)
6. [API Endpoints](#api-endpoints)
7. [Database Schema](#database-schema)
8. [Known Issues & Fixes](#known-issues--fixes)
9. [CHANGELOG](#changelog)
10. [Debugging Guide](#debugging-guide)

---

## SYSTEM OVERVIEW

Echo V3 is a **local-first AI assistant** with:
- **Offline chat** via MLX (local LLM inference)
- **Semantic memory** via embeddings
- **Calendar sync** via BetterShift
- **Real-time news** via RSS aggregators
- **Web browser automation** via webctl
- **Voice I/O** via Whisper + text-to-speech

### Architecture Type
**Microservices + Monolithic Hybrid**
- **Web server** (Flask/Starlette): Single monolith handling UI + API
- **MLX server** (Uvicorn): Separate inference engine
- **BetterShift** (Node.js/Next.js): Calendar system
- **webctl** (Subprocess): Browser automation daemon

### Key Design Principles
1. **Stateless HTTP** between services
2. **Async-first** for I/O operations
3. **Token efficiency** (50-line truncation max)
4. **Graceful fallbacks** on service failures
5. **Single session** across all devices

---

## CORE ARCHITECTURE

### System Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  USER INTERFACE                      ‚îÇ
‚îÇ  (HTML/CSS/JS + HTMX for real-time updates)        ‚îÇ
‚îÇ  http://127.0.0.1:5001                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          WEB SERVER (web_server.py)                 ‚îÇ
‚îÇ  Port: 5001 | Framework: Starlette + HTMX         ‚îÇ
‚îÇ  Responsibilities:                                  ‚îÇ
‚îÇ  - Render UI templates                             ‚îÇ
‚îÇ  - Route HTTP requests                             ‚îÇ
‚îÇ  - Call LLM client for chat processing             ‚îÇ
‚îÇ  - Manage sessions & chat history                  ‚îÇ
‚îÇ  - Forward to BetterShift proxy                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ           ‚îÇ           ‚îÇ
     ‚ñº           ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MLX     ‚îÇ ‚îÇDatabase  ‚îÇ ‚îÇBetterShift
‚îÇ Server  ‚îÇ ‚îÇ(SQLite)  ‚îÇ ‚îÇ Proxy
‚îÇ :1234   ‚îÇ ‚îÇTasks,    ‚îÇ ‚îÇ :3000
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇNotes,    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇHistory   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Service Dependencies

| Service | Port | Language | Role | Startup Order |
|---------|------|----------|------|---|
| webctl daemon | - | CLI | Browser automation | 1st |
| MLX Server | 1234 | Python | LLM inference | 2nd |
| Echo Web | 5001 | Python | UI + routing | 3rd |
| BetterShift | 3000 | Node.js | Calendar management | 4th |

---

## SERVICES & PORTS

### 1. MLX Server (Port 1234)
**File:** `mlx_server.py`  
**Framework:** Starlette + Uvicorn  
**Purpose:** Local LLM inference

#### Key Endpoints
- `POST /v1/chat/completions` - Generate text responses
- `GET /v1/models` - List available models + show which is selected
- `POST /v1/models/swap` - Load different model
- `GET /health` - Get current model + memory usage

#### State Variables
```python
chat_model = None            # Loaded model object
chat_tokenizer = None        # Tokenizer for current model
current_chat = DEFAULT_CHAT  # Path to loaded model
current_embed = DEFAULT_EMBED # Path to embedding model
```

#### Model Loading Flow
```
1. Read user_config.json for saved model path
2. If path invalid, fall back to DEFAULT_CHAT
3. Load model with mlx_lm.load()
4. Set tokenizer with 64k context max
5. Save new config to user_config.json
6. On swap failure, restore previous model or default
```

---

### 2. Echo Web Server (Port 5001)
**File:** `web_server.py`  
**Framework:** Starlette + HTMX  
**Purpose:** UI rendering + request routing

#### Key Endpoints
- `GET /` - Chat page
- `POST /send` - Process user message (calls llm_client.process_input)
- `GET /tasks` - Task management UI
- `GET /memory` - Memory/notes view
- `GET /schedule` - BetterShift calendar view
- `GET /models/active-badge` - Show current model + RAM
- `GET /models/selector` - Show model dropdown (fresh from MLX)
- `POST /models/swap` - Call MLX swap endpoint
- `POST /wake` - Start MLX server
- `POST /sleep` - Stop MLX server

#### Session Management
```python
SESSIONS = {}                    # Global dict: {session_id: history}
get_session_id(request)         # Always returns "echo-main" (single session)
database.get_chat_history(sid)  # Load history from DB on page load
```

---

### 3. Database (SQLite)
**File:** `echo.db`  
**Purpose:** Persistent storage

#### Tables

**tasks**
```sql
id INTEGER PRIMARY KEY
description TEXT
status TEXT (pending|completed|archived)
created_at TIMESTAMP
completed_at TIMESTAMP
archived_at TIMESTAMP
```

**notes**
```sql
id INTEGER PRIMARY KEY
content TEXT
embedding BLOB (numpy array serialized)
created_at TIMESTAMP
```

**chat_history**
```sql
id INTEGER PRIMARY KEY
session_id TEXT
role TEXT (user|assistant)
content TEXT
created_at TIMESTAMP
```

---

## DATA FLOW DIAGRAMS

### Chat Message Flow (Complete)

```
User Types Message
        ‚îÇ
        ‚ñº
Browser JS: sendMsg()
  ‚îú‚îÄ Add user message to DOM
  ‚îú‚îÄ Show "Thinking..." bubble
  ‚îî‚îÄ POST /send with message
        ‚îÇ
        ‚ñº
web_server.py: send_message()
  ‚îú‚îÄ Extract message from form
  ‚îú‚îÄ Load session history from DB
  ‚îú‚îÄ Call llm_client.process_input(message, history)
        ‚îÇ
        ‚ñº
llm_client.py: process_input()
  ‚îú‚îÄ CHECK FAST-PATHS (no LLM needed)
  ‚îÇ  ‚îú‚îÄ Task query? ‚Üí return list_tasks()
  ‚îÇ  ‚îú‚îÄ News query? ‚Üí return formatted news
  ‚îÇ  ‚îú‚îÄ "Complete task X"? ‚Üí deterministic completion
  ‚îÇ  ‚îú‚îÄ "Who is working"? ‚Üí check_entity_status()
  ‚îÇ  ‚îî‚îÄ BetterShift query? ‚Üí direct tool call
  ‚îÇ
  ‚îú‚îÄ BUILD LLM PROMPT
  ‚îÇ  ‚îú‚îÄ System prompt + context + tools
  ‚îÇ  ‚îú‚îÄ History (last 10 messages)
  ‚îÇ  ‚îî‚îÄ Current user message
  ‚îÇ
  ‚îú‚îÄ REASONING LOOP (max 6 turns)
  ‚îÇ  ‚îú‚îÄ Call MLX /v1/chat/completions
  ‚îÇ  ‚îú‚îÄ Parse for Tool: name(args) pattern
  ‚îÇ  ‚îú‚îÄ If tool found:
  ‚îÇ  ‚îÇ   ‚îî‚îÄ Execute tool ‚Üí observation
  ‚îÇ  ‚îÇ   ‚îî‚îÄ Feed back to LLM
  ‚îÇ  ‚îî‚îÄ If no tool ‚Üí return final answer
  ‚îÇ
  ‚îî‚îÄ Return answer to web_server
        ‚îÇ
        ‚ñº
web_server.py: render response
  ‚îú‚îÄ Save message + response to DB history
  ‚îú‚îÄ Return HTML message bubble
        ‚îÇ
        ‚ñº
Browser: Replace "Thinking..." with response
```

### Model Swap Flow (With Fix)

```
User Selects Model in Dropdown
        ‚îÇ
        ‚ñº
Browser JS: swapModel(modelPath)
  ‚îî‚îÄ POST /models/selector endpoint
        ‚îÇ
        ‚ñº
web_server.py: get_model_selector()
  ‚îî‚îÄ Fetch /v1/models from MLX
  ‚îî‚îÄ User clicks: onchange="swapModel(this.value)"
  ‚îî‚îÄ JS calls: fetch('/v1/models/swap', {model_path, type: 'chat'})
        ‚îÇ
        ‚ñº
mlx_server.py: swap_model()
  ‚îú‚îÄ Save old model path
  ‚îú‚îÄ Call load_chat(new_path)
  ‚îú‚îÄ If load fails:
  ‚îÇ  ‚îú‚îÄ Try restore old model
  ‚îÇ  ‚îú‚îÄ If restore fails, load DEFAULT_CHAT
  ‚îÇ  ‚îî‚îÄ Return error message
  ‚îî‚îÄ If success, return OK
        ‚îÇ
        ‚ñº
Browser JS (in get_model_selector HTML):
  ‚îú‚îÄ IMMEDIATE: htmx.ajax to /models/active-badge
  ‚îÇ  ‚îî‚îÄ Updates badge with new model name + RAM
  ‚îî‚îÄ AFTER 2s: htmx.ajax to /models/selector again
     ‚îî‚îÄ Refreshes dropdown to match actual state
```

---

## MODULE REFERENCE

### llm_client.py (1069 lines)
**Core Intelligence Engine**

#### Main Function: `process_input(user_input, history=[])`
```python
# Execution order:
1. Check 10 fast-path patterns (no LLM)
   - Task queries
   - News queries
   - Task completion by description
   - Who is working queries
   - BetterShift queries

2. Build LLM prompt:
   - System prompt + tools + context
   - Last 10 history messages (truncated if >2000 chars)
   - Current user input

3. ReAct loop (max 6 turns):
   FOR turn in range(6):
     - Call MLX chat endpoint
     - Parse for "Tool: name(args)"
     - Execute tool ‚Üí get observation
     - Feed observation back to LLM
     - Loop continues if tool called
     - Break if no tool (final answer)

4. Return final answer
```

#### Context Truncation: `_truncate_context(text, max_lines=50)`
**Purpose:** Prevent context overflow to LLM
```python
- Split text by newlines
- Keep first 50 lines
- Append "[... N more lines truncated ...]"
- Prevents thinking loops in reasoning models
```

#### Tool Execution: `execute_tool(name, args)`
**Available Tools:**
| Tool | Args | Purpose |
|------|------|---------|
| `add_task` | description | Add to-do item |
| `list_tasks` | - | List pending tasks |
| `complete_task` | task_id | Mark done by ID |
| `complete_task_by_description` | description | Mark done by text match |
| `delete_task_by_description` | description | Delete by text match |
| `get_news` | topic (opt) | Fetch formatted news |
| `save_note` | content | Save to memory |
| `recall_notes` | query | Search memory by embedding |
| `browse_web` | query | Use webctl for web automation |
| `check_entity_status` | - | Who is working now |
| `list_calendars` | - | Get BetterShift calendars |
| `create_shift` | calendar_id, title, date, ... | Add shift to calendar |

---

### web_server.py (2176 lines)
**HTTP Routing + UI Rendering**

#### Session Architecture
```python
SESSIONS = {}  # Global dict, keyed by session_id
get_session_id(request) ‚Üí "echo-main"  # Always same ID

# On chat page load:
history = database.get_chat_history("echo-main")
SESSIONS["echo-main"] = history

# On message send:
response = llm_client.process_input(msg, history=SESSIONS[sid])
```

#### Key Functions
- `get_active_model_badge()` - Fresh model + RAM from MLX health
- `get_model_selector()` - Fresh dropdown + swap handler
- `send_message()` - Process chat message
- `render_chat_view()` - Generate chat HTML
- `render_tasks_view()` - Task management UI
- `render_memory_view()` - Notes/memory display

---

### database.py
**Persistent Storage Layer**

#### Key Functions
- `init_db()` - Create tables if not exist
- `add_task(description)` - Insert new task
- `list_tasks(status)` - Query tasks by status
- `complete_task(task_id)` - Mark done
- `complete_task_by_description(description)` - Mark newest matching pending task
- `delete_task_by_description(description)` - Delete newest matching task
- `archive_completed_tasks()` - Mark completed tasks as 'archived' (not deleted)
- `get_archived_tasks()` - Retrieve all archived tasks for display
- `add_note(content, embedding)` - Save with embedding vector
- `get_similar_notes(query_embedding, top_k)` - Vector search
- `get_chat_history(session_id)` - Load conversation

---

### mlx_embeddings.py
**Semantic Search via Embeddings**

```python
# Embedding model: all-MiniLM-L6-v2-bf16 (384-dim vectors)

def get_embedding(text) ‚Üí numpy array
  - Tokenize text
  - Pass through encoder
  - Return 384-dim vector

# Used for:
- Semantic note search (recall_notes tool)
- Finding relevant memories
- Entity context retrieval
```

---

## API ENDPOINTS

### MLX Server (`http://127.0.0.1:1234`)

#### POST /v1/chat/completions
```json
Request:
{
  "messages": [
    {"role": "system", "content": "You are Echo..."},
    {"role": "user", "content": "hello"}
  ],
  "max_tokens": 600,
  "temperature": 0.3
}

Response:
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "Hi there! How can I help?"
    }
  }],
  "model": "/path/to/loaded/model"
}
```

#### GET /v1/models
```json
Response:
{
  "data": {
    "chat": [
      {
        "id": "/path/to/Llama-3.2-3B-Instruct-4bit",
        "name": "Llama-3.2-3B-Instruct-4bit",
        "selected": true
      },
      {...}
    ],
    "embed": [...]
  }
}
```

#### POST /v1/models/swap
```json
Request:
{
  "model_path": "/path/to/model",
  "type": "chat"
}

Response:
{
  "status": "ok|error",
  "message": "Description"
}

# On failure, automatically restores previous model
```

#### GET /health
```json
Response:
{
  "status": "ok",
  "chat_model": "/path/to/loaded/model",
  "embed_model": "/path/to/embedding",
  "memory_mb": 8234.5,
  "memory_gb": 8.05
}
```

### Echo Web Server (`http://127.0.0.1:5001`)

#### POST /send
```
Form Data:
  msg: "user message"

Returns: HTML message bubble with AI response
```

#### GET /models/active-badge
```
Returns: HTML badge showing current model + RAM
         Polls MLX /health directly (fresh data)
```

#### GET /models/selector
```
Returns: HTML select dropdown + swap handler JS
         Polls MLX /v1/models directly (fresh data)
```

---

## DATABASE SCHEMA

### tasks
```sql
CREATE TABLE tasks (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  description TEXT NOT NULL,
  status TEXT DEFAULT 'pending',  -- pending, completed, archived
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  completed_at TIMESTAMP,
  archived_at TIMESTAMP
);

-- Indices
CREATE INDEX idx_status ON tasks(status);
CREATE INDEX idx_created_at ON tasks(created_at DESC);
```

### notes
```sql
CREATE TABLE notes (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  content TEXT NOT NULL,
  embedding BLOB,  -- Serialized numpy array (384 floats)
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Indices
CREATE INDEX idx_created_at ON notes(created_at DESC);
```

### chat_history
```sql
CREATE TABLE chat_history (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  session_id TEXT NOT NULL,
  role TEXT NOT NULL,  -- user, assistant
  content TEXT NOT NULL,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Indices
CREATE INDEX idx_session ON chat_history(session_id, created_at);
```

---

## KNOWN ISSUES & FIXES

### FIXED: Issue #1 - Model State Mismatch (Badge ‚â† Dropdown)

**Symptom:**
- Active badge shows: "ü§ñ Phi-4"
- Dropdown shows: "Qwen3-VL" (stale)
- User confused about which model is actually loaded

**Root Cause:**
- Badge called `/health` (always fresh from MLX)
- Dropdown called `/models` (web_server cached response)
- After model swap, MLX updated but web_server cache didn't refresh

**Solution:**
1. Renamed routes:
   - `/models` ‚Üí `/models/selector` (get_model_selector)
   - `/models/active` ‚Üí `/models/active-badge` (get_active_model_badge)

2. Both endpoints now call MLX directly (no caching):
   ```python
   # get_active_model_badge():
   - GET http://127.0.0.1:1234/health
   - Extract chat_model + memory_mb
   - Return fresh HTML badge
   
   # get_model_selector():
   - GET http://127.0.0.1:1234/v1/models
   - Build dropdown with "selected" attribute from MLX
   - Include JS: on dropdown change ‚Üí POST /v1/models/swap
   ```

3. Auto-refresh on swap:
   ```javascript
   async function swapModel(modelPath) {
     // 1. POST swap to MLX
     // 2. IMMEDIATELY refresh badge (0s)
     // 3. AFTER 2s refresh dropdown (wait for load)
   }
   ```

**Files Changed:**
- `web_server.py`: lines 1274-1384 (new endpoints)
- `web_server.py`: lines 822-829 (UI HTML)
- `web_server.py`: lines 2163-2164 (routes)

**Test Result:** ‚úÖ PASS - Badge and dropdown now always sync

---

### FIXED: Issue #2 - Archive Function Deleted Tasks Instead of Archiving

**Symptom:**
- User completes tasks and clicks "Archive" button
- Completed tasks disappear permanently
- Archives page shows no archived tasks (always empty)

**Root Cause:**
- `archive_chat()` function in web_server.py called `database.delete_completed_tasks()` 
- This permanently deleted completed tasks from database
- `get_archived_tasks()` only retrieves tasks with status='archived'
- Since tasks were deleted (not archived), Archives page was always empty

**Solution:**
1. Changed `archive_chat()` to call `archive_completed_tasks()` instead:
   ```python
   # BEFORE (‚ùå Deletes permanently)
   database.delete_completed_tasks()
   
   # AFTER (‚úÖ Marks as 'archived')
   database.archive_completed_tasks()
   ```

2. Fixed database functions returning incorrect values:
   - All delete/update functions checked `conn.total_changes` BEFORE `conn.commit()`
   - This always returned 0 because changes haven't been committed yet
   - Changed to `cursor.rowcount` checked AFTER `conn.commit()`
   - Affected functions: `delete_note()`, `delete_task()`, `complete_task()`, `delete_completed_tasks()`, `complete_all_tasks()`, `archive_completed_tasks()`

3. Added security fixes:
   - Path traversal protection in `delete_archive_file()` using `os.path.basename()`
   - File extension validation (only .txt files allowed)
   - Error handling for corrupted/missing archive files

4. Added confirmation dialog:
   - Archive button now shows `hx-confirm` dialog before destructive action

**Files Changed:**
- `database.py`: Lines 146-273 (6 functions fixed with cursor.rowcount)
- `web_server.py`: Line 2140 (archive_completed_tasks instead of delete)
- `web_server.py`: Lines 2009-2024 (security fix for path traversal)
- `web_server.py`: Lines 1390-1440 (error handling for file reads)
- `web_server.py`: Line 984 (confirmation dialog)

**Test Result:** ‚úÖ PASS - 27/27 tests passed including:
- Database functions return correct values
- Archived tasks appear in Archives page
- Security tests pass (path traversal blocked)
- Error handling graceful for corrupted files

---

### FIXED: Issue #3 - Follow-Up News Questions Hang

**Symptom:**
- Turn 1: "latest news" ‚Üí ‚úÖ Works
- Turn 2: "what about the cyclone?" ‚Üí ‚è≥ Hangs or returns generic response

**Root Cause:**
- `get_news()` returned raw list of dicts
- News list not formatted with "Cyclone Dudzai" explicitly
- LLM had no context for pronoun "the cyclone"
- LLM tried to infer ‚Üí thinking loop ‚Üí timeout

**Solution:**
1. Enhanced news formatting in `llm_client.py`:
   ```python
   # Before:
   return await news_aggregator.get_daily_news(topic)  # Returns list
   
   # After:
   news_list = await news_aggregator.get_daily_news(topic)
   formatted = "üìÖ Daily News Headlines:\n"
   for i, item in enumerate(news_list[:10], 1):
     headline = item.get('headline', '')
     source = item.get('source', '')
     formatted += f"{i}. [{source}] {headline}\n"
   return _truncate_context(formatted, max_lines=50)
   ```

2. Result: News in history now reads:
   ```
   üìÖ Daily News Headlines:
   1. [Lemauricien] Cyclone Dudzai: Air Mauritius suspends vols...
   2. [Mbcradio] Sports Update...
   ```

3. Follow-up "what about the cyclone?" now sees explicit "Cyclone Dudzai" text

**Files Changed:**
- `llm_client.py`: lines 741-760 (enhanced get_news handler)

**Test Result:** ‚úÖ PASS - Follow-ups now work correctly

---

## CHANGELOG

### [3.0.0] - 2026-01-18 - PRODUCTION RELEASE

#### Added
- ‚úÖ Webctl browser automation daemon (startup integrated)
- ‚úÖ Deterministic task completion/deletion by description
- ‚úÖ Model state sync fix (badge ‚â† dropdown resolved)
- ‚úÖ News formatting for LLM context
- ‚úÖ 3-tier fallback on model swap failure
- ‚úÖ Token-efficient snapshots (50-line max)
- ‚úÖ Multi-turn context retention (10-message window)

#### Fixed
- ‚úÖ Issue #1: Active badge vs dropdown model mismatch
- ‚úÖ Issue #2: Follow-up news questions hanging
- ‚úÖ LFM2.5 tokenizer compatibility
- ‚úÖ Entity resolution for team references

#### Changed
- ‚úÖ History window: 6 messages ‚Üí 10 messages
- ‚úÖ News output: raw list ‚Üí formatted with sources
- ‚úÖ Model selector: cached `/models` ‚Üí fresh `/models/selector`
- ‚úÖ Active badge: `/models/active` ‚Üí `/models/active-badge`

#### Performance
- ‚ö° 75% reduction in snapshot tokens (2-3K ‚Üí 500-800)
- ‚ö° Model swap fallback: zero downtime
- ‚ö° Chat latency: unchanged (still <500ms)

---

## DEBUGGING GUIDE

### Common Issues & Fixes

#### "LLM Error: 500"
**Cause:** MLX server not responding or model failed to load

**Debug Steps:**
1. Check MLX is running: `curl http://127.0.0.1:1234/health`
2. Check logs: `tail -n 50 .run_all_logs/mlx_server.log`
3. Verify model path exists: `ls -la models/chat/[model_name]/config.json`
4. Restart MLX: `./run_all.sh restart`

**Common Fixes:**
- Model path broken ‚Üí fallback to DEFAULT_CHAT
- Out of memory ‚Üí reduce max_tokens or swap to smaller model
- Tokenizer issue ‚Üí check tokenizer_config.json

---

### Model State Mismatch (Badge ‚â† Dropdown)
**Symptom:** Active badge shows Phi-4, dropdown shows Qwen

**Debug Steps:**
1. Open browser DevTools ‚Üí Network tab
2. Look at `/models/active-badge` response (should show Phi-4)
3. Look at `/models/selector` response (should show Qwen selected=true, Phi-4 selected=false until after swap)
4. Check if both endpoints are calling MLX directly:
   ```python
   resp = await client.get("http://127.0.0.1:1234/health")  # active-badge
   resp = await client.get("http://127.0.0.1:1234/v1/models")  # selector
   ```

**Fix:** Ensure routes match those in `web_server.py` lines 2163-2164

---

### Follow-Up Question Hangs
**Symptom:** "what about X" returns nothing or times out

**Debug Steps:**
1. Check news is formatted: `grep "üìÖ Daily News" test output`
2. Check history contains explicit headline text:
   ```python
   # Should see in history:
   "üìÖ Daily News Headlines:\n1. [Source] Specific topic..."
   ```
3. Verify truncation: `_truncate_context(news, max_lines=50)` applied
4. Check LLM response time: `tail .run_all_logs/web_server.log | grep "Trace (Turn"`

**Fix:** Ensure `get_news()` formats with numbered list + sources (see Issue #2 fix)

---

### Database Corruption or Missing History
**Symptom:** Chat history not persisting or appears empty

**Debug Steps:**
1. Check DB exists: `ls -la echo.db`
2. Inspect tables: `sqlite3 echo.db "SELECT COUNT(*) FROM chat_history;"`
3. Check session ID: `grep "get_session_id" web_server.py` (should always be "echo-main")
4. Verify history loaded: `sqlite3 echo.db "SELECT * FROM chat_history ORDER BY created_at DESC LIMIT 5;"`

**Fix:**
- Delete `echo.db` to reset (will recreate on first run)
- Check database.py `init_db()` is called on startup (web_server line 21)

---

### Service Port Conflicts
**Symptom:** "Address already in use" on startup

**Debug Steps:**
1. Check what's using the port: `lsof -i :5001` (Echo), `:1234` (MLX), `:3000` (BetterShift)
2. Kill process: `kill -9 PID`
3. Or restart all: `./run_all.sh restart`

---

### Tool Execution Failures
**Symptom:** "Tool Execution Error" in response

**Debug Steps:**
1. Check which tool failed: look at error message
2. Verify tool arguments match signature in `execute_tool()`
3. Check prerequisites:
   - `list_tasks` requires tasks table
   - `recall_notes` requires embeddings model loaded
   - `check_entity_status` requires BetterShift running

**Fix:** Inspect llm_client.py `execute_tool()` function for specific tool, add logging

---

## DEVELOPMENT WORKFLOW

### Adding a New Feature

1. **Understand the flow:**
   - Read this document for context
   - Trace the data flow from user input to response
   - Identify which module(s) need changes

2. **Implement:**
   - Update `llm_client.py` if adding tool or LLM logic
   - Update `web_server.py` if adding endpoint or UI
   - Update `database.py` if adding persistence

3. **Test:**
   - Run `python3 tests/run_all_tests.py`
   - Manually test feature via UI
   - Check logs for errors

4. **Document:**
   - Update this `architecture.md` file:
     - Add to CHANGELOG section
     - Update relevant module reference
     - Add to debugging guide if needed
   - Update function docstrings

5. **Commit:**
   - Use clear commit messages
   - Reference this doc in commit message

---

### Extending a Tool

Example: Adding `get_weather` tool

1. **Add tool definition** in `llm_client.py` TOOLS_PROMPT (line ~28)
2. **Implement handler** in `execute_tool()` function (line ~650)
3. **Add fast-path** if it should bypass LLM (line ~212)
4. **Test** via UI
5. **Document** in this file (Module Reference + Changelog)

---

## CONTACT & SUPPORT

For questions or issues:
1. **Check this document first** - contains 95% of answers
2. **Review CHANGELOG** - recent fixes might apply
3. **Check logs** - `.run_all_logs/` directory
4. **Search codebase** - use grep for patterns
5. **Add debugging** - add print statements, rerun tests

---

**End of Architecture Document**  
*This file is the source of truth for all system knowledge. Keep it updated.*
